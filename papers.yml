- abstract: Transformer models have revolutionized NLP, yet many morphologically rich
    languages remain underrepresented in large-scale pre-training efforts. With SindBERT,
    we set out to chart the seas of Turkish NLP, providing the first large-scale RoBERTa-based
    encoder for Turkish. Trained from scratch on 312 GB of Turkish text (mC4, OSCAR23,
    Wikipedia), SindBERT is released in both base and large configurations, representing
    the first large-scale encoder-only language model available for Turkish. We evaluate
    SindBERT on part-of-speech tagging, named entity recognition, offensive language
    detection, and the TurBLiMP linguistic acceptability benchmark. Our results show
    that SindBERT performs competitively with existing Turkish and multilingual models,
    with the large variant achieving the best scores in two of four tasks but showing
    no consistent scaling advantage overall. This flat scaling trend, also observed
    for XLM-R and EuroBERT, suggests that current Turkish benchmarks may already be
    saturated. At the same time, comparisons with smaller but more curated models
    such as BERTurk highlight that corpus quality and diversity can outweigh sheer
    data volume. Taken together, SindBERT contributes both as an openly released resource
    for Turkish NLP and as an empirical case study on the limits of scaling and the
    central role of corpus composition in morphologically rich languages. The SindBERT
    models are released under the MIT license and made available in both fairseq and
    Huggingface formats.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: ''
  authors:
  - dblp_id: https://dblp.org/pid/258/2829.html
    emails: '****@tum.de'
    first_name: Raphael
    google_scholar_id: https://scholar.google.de/citations?user=gQ2pZf8AAAAJ
    homepage: https://raphiniert.com
    institution: Medical Center - University of Freiburg and Technische Universität
      München
    last_name: Schmitt
    name: Raphael Schmitt
    orcid: https://orcid.org/0000-0003-2067-5569
    semantic_scholar_id: https://www.semanticscholar.org/author/Raphael-Scheible/12342392
    username: ~Raphael_Schmitt1
  - dblp_id: https://dblp.org/pid/242/7357
    emails: '****@schweter.eu'
    first_name: Stefan
    homepage: https://github.com/stefan-it
    institution: Bavarian State Library
    last_name: Schweter
    name: Stefan Schweter
    orcid: https://orcid.org/0000-0002-7190-2090
    semantic_scholar_id: https://www.semanticscholar.org/author/Stefan-Schweter/134757625
    username: ~Stefan_Schweter1
  decision: EACL
  file: 1.pdf
  id: 1
  openreview_id: rM46NUDv7w
  pdf_file: 85ed4119c44cb33879293eb468c95ef2653025a0.pdf
  title: 'SindBERT, the Sailor: Charting the Seas of Turkish NLP'
- abstract: Authorial style transfer is particularly challenging in low-resource scenarios,
    such as those presented by languages with a distinct socio-digital trajectory
    like Turkish, where contemporary digital text coexists with under-resourced literary
    and historical styles. This work addresses this gap through the Dual-Stage Stylometric
    Imprinting (DSSI) framework, introducing a Rule+Example paradigm for effective
    style profiling. Evaluated on a corpus of Turkish texts, the approach enables
    smaller models to achieve up to 90% of large model performance by combining explicit
    stylistic guidelines with contextual demonstrations. The findings demonstrate
    altered scaling laws for stylistic tasks and facilitate the practical deployment
    of personalized style transfer for preserving distinctive writing characteristics.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: ''
  authors:
  - emails: '****@std.bogazici.edu.tr'
    first_name: Hüseyin
    last_name: Akdağ
    middle_name: Emir
    name: Hüseyin Emir AKDAĞ
    username: ~Hüseyin_Emir_AKDAĞ1
  decision: EACL
  file: 3.pdf
  id: 3
  openreview_id: UGAK60cSJy
  pdf_file: fbd49589855d2eec3cab3938564896fc8ad2cbb5.pdf
  title: 'Directed Attention is All You Need: Profiling Style from Limited Text Data'
- abstract: 'Most large language models (LLMs) are trained

    on massive datasets that include private infor-

    mation, which may be disclosed to third-party

    users in output generation. Developers put de-

    fences to prevent the generation of harmful and

    private information, but jailbreaking methods

    can be used to bypass them. Machine unlearn-

    ing aims to remove information that may be

    private or harmful from the model’s genera-

    tion without retraining the model from scratch.

    While machine unlearning has gained some

    popularity to counter the removal of private

    information, especially in English, little to no

    attention has been given to Turkish unlearn-

    ing paradigms or existing benchmarks. In this

    study, we introduce TUNE (Turkish Unlearn-

    ing Evaluation), the first benchmark dataset

    for Turkish unlearning task for personal infor-

    mation. TUNE consists of 9842 input-target

    text pairs about 50 fictitious personalities with

    two training task types: (1) Q&A and (2) In-

    formation Request. We fine-tuned the mT5

    base model to evaluate various unlearning meth-

    ods, including our proposed approach. We find

    that while current methods can help unlearn

    unwanted private information in Turkish, they

    also unlearn other information we want to re-

    tain in the model.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: ''
  authors:
  - emails: '****@sabanciuniv.edu'
    first_name: Doruk
    institution: Sabanci University
    last_name: Benli
    name: Doruk Benli
    username: ~Doruk_Benli1
  - emails: '****@sabanciuniv.edu'
    first_name: Ada
    institution: Sabanci University
    last_name: Canoğlu
    name: Ada Canoğlu
    username: ~Ada_Canoğlu1
  - emails: '****@sabanciuniv.edu'
    first_name: Nehir
    last_name: Gönençer
    middle_name: İlkim
    name: Nehir İlkim Gönençer
    username: ~Nehir_İlkim_Gönençer1
  - dblp_id: https://dblp.org/pid/184/8318
    emails: '****@sabanciuniv.edu'
    first_name: Dilara
    google_scholar_id: https://scholar.google.com/citations?user=b6-c_A0AAAAJ
    homepage: https://dilarakkl.github.io
    institution: Sabanci University
    last_name: Keküllüoğlu
    name: Dilara Keküllüoğlu
    orcid: https://orcid.org/0000-0003-1214-9876
    username: ~Dilara_Keküllüoğlu1
  decision: EACL
  file: 5.pdf
  id: 5
  openreview_id: gbr5dKu80S
  pdf_file: 7011a444a5c9f78cf0e20d8f84d5b971947d616e.pdf
  title: 'TUNE: A Task For Turkish Machine Unlearning For Data Privacy'
- abstract: Idiomatic expressions are culturally grounded, semantically opaque, and
    difficult to interpret for multilingual natural language processing systems. Despite
    the large speaker population of Turkic languages, resources that focus on monolingual
    and cross-lingual idioms and their meanings are limited. We introduce the first
    unified benchmark for idiom understanding across Turkish, Azerbaijani, Turkmen,
    Gagauz, and Uzbek languages. The datasets compiled include token-level idiom span
    annotations. We develop models for idiom identification and semantic retrieval
    tasks. We evaluate seven models for idiom identification and nine embedding models
    for semantic retrieval tasks under several fine-tuning schemes using standard
    dense retrieval metrics. This benchmark provides a basis for studying idiomatic
    phenomena in Turkic languages and clarifies how idiomatic meanings are shared,
    altered, or diverge across languages.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: ''
  authors:
  - emails: '****@std.bogazici.edu.tr'
    first_name: Gözde
    institution: Bogazici University
    last_name: Aslantaş
    name: Gözde Aslantaş
    orcid: https://orcid.org/0000-0002-8887-1796
    username: ~Gözde_Aslantaş1
  - dblp_id: https://dblp.org/pid/12/4512
    emails: '****@bogazici.edu.tr'
    first_name: Tunga
    google_scholar_id: https://scholar.google.com/citations?user=oymuGoYAAAAJ&hl=en&oi=ao
    homepage: https://www.cmpe.boun.edu.tr/~gungort/
    institution: Bogazici University
    last_name: Gungor
    name: Tunga Gungor
    orcid: https://orcid.org/0000-0001-9448-9422
    semantic_scholar_id: https://www.semanticscholar.org/author/Tunga-G%C3%BCng%C3%B6r/1765713
    username: ~Tunga_Gungor1
  decision: EACL
  file: 7.pdf
  id: 7
  openreview_id: rzn72LFzd4
  pdf_file: 13c8fe67fb604ee0adce4cb3818c84ba7368b6a4.pdf
  title: 'A Unified Turkic Idiom Understanding Benchmark: Idiom Detection and Semantic
    Retrieval Across Five Turkic Languages'
- abstract: This study presents a framework for generating the gold-standard summary
    fully automatically and reproducibly based on multiple human summaries of Turkish
    educational videos. Within the scope of the study, a new dataset called TR-EduVSum
    was created, encompassing 82 Turkish course videos in the field of "Data Structures
    and Algorithms" and containing a total of 3281 independent human summaries. Inspired
    by existing pyramid-based evaluation approaches, the AutoMUP (Automatic Meaning
    Unit Pyramid) method is proposed, which extracts consensus-based content from
    multiple human summaries. AutoMUP clusters the meaning units extracted from human
    summaries using embedding, statistically models inter-participant agreement, and
    generates graded summaries based on consensus weight. In this framework, the gold
    summary corresponds to the highest-consensus AutoMUP configuration, constructed
    from the most frequently supported meaning units across human summaries. Experimental
    results show that AutoMUP summaries exhibit high semantic overlap with robust
    LLM summaries such as Flash 2.5 and GPT-5.1. Furthermore, ablation studies clearly
    demonstrate the decisive role of consensus weight and clustering in determining
    summary quality. The proposed approach can be generalized to other Turkic languages
    at low cost.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: ''
  authors:
  - emails: '****@ogr.ikc.edu.tr'
    first_name: Figen
    last_name: Eğin
    name: Figen Eğin
    orcid: https://orcid.org/0000-0003-4865-5789
    username: ~Figen_Eğin1
  - emails: aytugonan@gmail.com
    first_name: Aytuğ
    institution: Izmir Institute of Technology
    last_name: Onan
    name: aytugonan@gmail.com
    username: aytugonan@gmail.com
  decision: EACL
  file: 8.pdf
  id: 8
  openreview_id: 0EEWtohQDV
  pdf_file: c3887a9656fc2990df35acaa0756942c5662b63f.pdf
  title: 'TR-EduVSum: A Turkish-Focused Dataset and Consensus Framework for Educational
    Video Summarization'
- abstract: Sarcasm is a colloquial form of language that is used to convey messages
    in a non-literal way, which affects the performance of many NLP tasks. Sarcasm
    detection is not trivial and existing work mainly focus on only English. We present
    SarcasTürk, a context-aware Turkish sarcasm detection dataset built from Ekşi
    Sözlük entries, a large-scale Turkish online discussion platform where people
    frequently use sarcasm. SarcasTürk contains 1,515 entries from 98 titles with
    binary sarcasm labels and a title-level context field created to support comparisons
    between entry-only and context-aware models. We generate these contexts by selecting
    representative sentences from all entries under a title using summarization techniques.
    We report baseline results for a fine-tuned BERTurk classifier and zero-shot LLMs
    under both no-context and context-aware conditions. We find that BERTurk model
    with title-level context has the best performance with 0.76 accuracy and balanced
    class-wise F1 scores (0.77 for sarcasm, 0.75 for no sarcasm). SarcasTürk can be
    shared upon contacting the authors since the dataset contains potentially sensitive
    and offensive language.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: ''
  authors:
  - emails: '****@sabanciuniv.edu'
    first_name: Niyazi
    google_scholar_id: https://scholar.google.com/citations?user=yKeXTqIAAAAJ&hl=en
    last_name: Metin
    middle_name: Ahmet
    name: Niyazi Ahmet Metin
    username: ~Niyazi_Ahmet_Metin1
  - emails: '****@sabanciuniv.edu'
    first_name: Sevde
    last_name: Yılmaz
    name: Sevde Yılmaz
    username: ~Sevde_Yılmaz1
  - emails: '****@sabanciuniv.edu'
    first_name: Osman
    last_name: Erdoğdu
    middle_name: Enes
    name: Osman Enes Erdoğdu
    username: ~Osman_Enes_Erdoğdu1
  - emails: '****@sabanciuniv.edu'
    first_name: Elif
    institution: Sabanci University
    last_name: Meydan
    middle_name: Sude
    name: Elif Sude Meydan
    username: ~Elif_Sude_Meydan1
  - emails: '****@sabanciuniv.edu'
    first_name: Oğul
    last_name: Sümer
    name: Oğul Sümer
    username: ~Oğul_Sümer1
  - dblp_id: https://dblp.org/pid/184/8318
    emails: '****@sabanciuniv.edu'
    first_name: Dilara
    google_scholar_id: https://scholar.google.com/citations?user=b6-c_A0AAAAJ
    homepage: https://dilarakkl.github.io
    institution: Sabanci University
    last_name: Keküllüoğlu
    name: Dilara Keküllüoğlu
    orcid: https://orcid.org/0000-0003-1214-9876
    username: ~Dilara_Keküllüoğlu1
  decision: EACL
  file: 10.pdf
  id: 10
  openreview_id: 4Mqew1APs7
  pdf_file: 902dfe42921560f56a67c353d8a40be56d6a0539.pdf
  title: 'SarcasTürk: Turkish Context-Aware Sarcasm Detection Dataset'
- abstract: We present, to our knowledge, the first systematic transformer-based outlet-ideology
    classification study for Turkish news. Using a topic-balanced corpus of Turkish
    political articles drawn from six outlets commonly perceived as left-, centre-,
    or right-leaning, we formulate a three-way outlet-ideology classification task.
    On this dataset, we evaluate a monolingual encoder (BERTurk), two multilingual
    encoders (mBERT, XLM-R), and a LoRA-adapted decoder model (Mistral). BERTurk achieves
    the best performance among individual models (70\% accuracy, 71\% macro-F1), reaching
    levels comparable to English-language studies despite operating in a lower-resource
    setting. Error analyses show that all encoders reliably distinguish centrist from
    partisan articles, but frequently confuse left- and right-leaning articles with
    each other. Moreover, BERTurk is relatively stronger on right-leaning content,
    whereas the multilingual models favour left-leaning content, suggesting an ``ideological
    fingerprint'' of their pre-training data. Crucially, models fine-tuned on an English
    political-bias task fail to transfer to Turkish, collapsing to near-chance performance.
    Taken together, these results demonstrate that effective political bias detection
    requires target-language supervision and cannot be achieved through na\"ive cross-lingual
    transfer. Our work establishes a first baseline for Turkish political bias detection
    and underscores the need for open, carefully designed Turkish (and broader Turkic)
    bias benchmarks to support robust and fair media analysis.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: ''
  authors:
  - emails: '****@qmul.ac.uk'
    first_name: Umut
    last_name: Ozbagriacik
    name: Umut Ozbagriacik
    username: ~Umut_Ozbagriacik1
  - dblp_id: https://dblp.org/pid/162/7521
    emails: '****@gmail.com'
    first_name: Haim
    google_scholar_id: https://scholar.google.com/citations?user=2EDsENQAAAAJ&hl=en
    institution: Queen Mary University of London
    last_name: Dubossarsky
    name: Haim Dubossarsky
    semantic_scholar_id: https://www.semanticscholar.org/author/Haim-Dubossarsky/2026652?sort=pub-date
    username: ~Haim_Dubossarsky1
  decision: EACL
  file: 13.pdf
  id: 13
  openreview_id: qbux75wkEf
  pdf_file: 2ba3fcaab0d493f85e516339e537a9f7d9d9b120.pdf
  title: 'Language Matters: Target-Language Supervision for Political Bias Detection
    in Turkish News'
- abstract: In this paper, we investigate how transformer models represent complex
    verb paradigms in Turkish and Modern Hebrew, focusing on how tokenization strategies
    shape this ability. Using the Blackbird Language Matrices task on natural data,
    we show that for Turkish—with its transparent morphological markers—both monolingual
    and multilingual models succeed either when tokenization is highly atomic or breaking
    words into small subword units. For Hebrew, however, a multilingual model using
    character-level tokenization fails to capture its non-concatenative morphology,
    while a monolingual model with unified morpheme-aware segmentation excels. Performance
    improves on more synthetic datasets, in all models.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: ''
  authors:
  - emails: '****@idiap.ch'
    first_name: Giuseppe
    google_scholar_id: https://scholar.google.com/citations?user=TaL7sQYAAAAJ&hl=it
    institution: Idiap Research Institute and Beijing Language and Culture University
    last_name: Samo
    name: Giuseppe Samo
    orcid: https://orcid.org/0000-0003-3449-8006
    username: ~Giuseppe_Samo1
  - dblp_id: https://dblp.org/pid/94/2907
    emails: '****@unige.ch'
    first_name: Paola
    google_scholar_id: https://scholar.google.com/citations?user=XI-2Nd8AAAAJ&hl=en
    homepage: https://www.unige.ch/lettres/linguistique/collaborateurs/collaborateurs-de-lenseignement-et-de-la-recherche/home/
    institution: Idiap Research Institute and University of Geneva, Switzerland
    last_name: Merlo
    name: Paola Merlo
    semantic_scholar_id: https://www.semanticscholar.org/author/Paola-Merlo/143939590
    username: ~Paola_Merlo1
  decision: EACL
  file: 14.pdf
  id: 14
  openreview_id: HkiafVx5do
  pdf_file: c498ec760f71ae8041c10f4bce9faaf6eb33b4d9.pdf
  title: 'Modelling the Morphology of Verbal Paradigms: A Case Study in the Tokenization
    of Turkish and Hebrew'
- abstract: Minimal pair benchmarks have become a common approach for evaluating the
    syntactic knowledge of language models (LMs). However, the creation of such benchmarks
    often overlooks language-specific confounders that may affect model performance,
    particularly in the case of morphologically rich languages. In this paper, we
    investigate how surface-level factors such as morpheme count, subword count, and
    sentence length influence the performance of LMs on a Turkish benchmark of linguistic
    minimal pairs. We further analyze whether a tokenizer's degree of alignment with
    morphological boundaries can serve as a proxy for model performance. Finally,
    we test whether the distribution of morphemes in a minimal pair benchmark can
    skew model performance. Our results show that while surface factors have limited
    predictive power, they might still serve as a systematic source of bias. Moreover,
    we find that morphological alignment can roughly correspond to model performance,
    and morpheme-level imbalances in the benchmark may have a significant influence
    on results.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: ''
  authors:
  - emails: '****@rug.nl'
    first_name: Ezgi
    institution: Université de Lorraine and University of Groningen
    last_name: Başar
    name: Ezgi Başar
    orcid: https://orcid.org/0009-0002-0879-0150
    username: ~Ezgi_Başar1
  - dblp_id: https://dblp.org/pid/32/10934
    emails: '****@rug.nl'
    first_name: Arianna
    google_scholar_id: https://scholar.google.it/citations?user=biQvUhcAAAAJ&hl=it
    homepage: https://www.cs.rug.nl/~bisazza/
    institution: University of Groningen
    last_name: Bisazza
    name: Arianna Bisazza
    semantic_scholar_id: https://www.semanticscholar.org/author/Arianna-Bisazza/3242253
    username: ~Arianna_Bisazza1
  decision: EACL
  file: 17.pdf
  id: 17
  openreview_id: wQwdln5ZS2
  pdf_file: 7a194e793c25de6a955d1c2b2d6da35d2b366d6d.pdf
  title: A Morphology-Aware Evaluation of Turkish Syntax in Large Language Models
- abstract: In this paper, we investigated the task of hate-speech classification
    in the closely related Turkic language pair, Turkish-Azerbaijani. Transformer
    models can achieve strong hate-speech classification in Turkish, but their performance
    does not reliably transfer to closely related low-resource languages without careful
    evaluation. We study Turkish–Azerbaijani hate speech detection and introduce the
    first manually annotated Azerbaijani benchmark, comprising 1,112 YouTube comments
    from major news channels with severe class imbalance. We compare XLM-RoBERTa and
    a compact BERT-Tiny model against a TF–IDF + logistic regression baseline under
    monolingual training, zero-shot Turkish→Azerbaijani transfer, low-resource balanced
    subsampling, bilingual mixed fine-tuning, and translation-based augmentation using
    machine-translated Turkish data. XLM-R attains high macro-F1 in Turkish and achieves
    moderate zero-shot transfer to Azerbaijani, but native Azerbaijani training is
    fragile for the hate class. Mixed bilingual training improves robustness for both
    languages, whereas TF–IDF generalizes poorly to Azerbaijani.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: ''
  authors:
  - emails: '****@qmul.ac.uk'
    first_name: Tural
    google_scholar_id: https://scholar.google.com/citations?user=HsKHYGYAAAAJ&hl=en&authuser=1
    last_name: Alizada
    name: Tural Alizada
    username: ~Tural_Alizada1
  - dblp_id: https://dblp.org/pid/162/7521
    emails: '****@gmail.com'
    first_name: Haim
    google_scholar_id: https://scholar.google.com/citations?user=2EDsENQAAAAJ&hl=en
    institution: Queen Mary University of London
    last_name: Dubossarsky
    name: Haim Dubossarsky
    semantic_scholar_id: https://www.semanticscholar.org/author/Haim-Dubossarsky/2026652?sort=pub-date
    username: ~Haim_Dubossarsky1
  decision: EACL
  file: 18.pdf
  id: 18
  openreview_id: 8EcwhzmfJv
  pdf_file: b39f620a213a9b9da05aeab6d036228753116da4.pdf
  title: Benchmarking Hate Speech Detection in Azerbaijani with Turkish Cross-Lingual
    Transfer and Transformer Models
- abstract: 'Euphemisms substitute socially sensitive expressions, often softening
    or reframing meaning, and their reliance on cultural and pragmatic context complicates
    modeling across languages. In this study, we investigate how cross-lingual equivalence
    influences transfer in multilingual euphemism detection. We categorize Potentially
    Euphemistic Terms (PETs) in Turkish and English into Overlapping (OPETs) and Non-Overlapping
    (NOPETs) subsets based on their functional, pragmatic, and semantic alignment.  Our
    findings reveal a transfer asymmetry: semantic overlap is insufficient to guarantee
    positive transfer, particularly in low-resource Turkish-to-English direction,
    where performance can degrade even for overlapping euphemisms, and in some cases,
    improve under NOPET-based training. Differences in label distribution help explain
    these counterintuitive results. Category-level analysis suggests that transfer
    may be influenced by domain-specific alignment, though evidence is limited by
    sparsity.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: ''
  authors:
  - emails: '****@montclair.edu'
    first_name: Hasan
    google_scholar_id: https://scholar.google.com/citations?user=NYMWirsAAAAJ&hl=en
    last_name: Biyik
    middle_name: Can
    name: Hasan Can Biyik
    username: ~Hasan_Can_Biyik1
  - dblp_id: https://dblp.org/pid/02/8159
    emails: '****@gmail.com'
    first_name: Libby
    institution: Montclair State University
    last_name: Barak
    name: Libby Barak
    orcid: https://orcid.org/0000-0003-1045-7701
    username: ~Libby_Barak1
  - dblp_id: https://dblp.org/pid/05/4333-1
    emails: '****@mail.montclair.edu'
    first_name: Jing
    google_scholar_id: https://scholar.google.com/citations?user=tfOnyFcAAAAJ&hl=en&oi=ao
    institution: Montclair State University
    last_name: Peng
    name: Jing Peng
    username: ~Jing_Peng1
  - dblp_id: https://dblp.org/pid/77/2786
    emails: '****@montclair.edu'
    first_name: Anna
    google_scholar_id: https://scholar.google.com/citations?user=1rNmLzYAAAAJ&hl=en
    homepage: https://msuweb.montclair.edu/~feldmana/
    institution: Montclair State University
    last_name: Feldman
    name: Anna Feldman
    orcid: https://orcid.org/0000-0003-4146-5990
    semantic_scholar_id: https://www.semanticscholar.org/author/Anna-Feldman/145754614
    username: ~Anna_Feldman1
  decision: EACL
  file: 19.pdf
  id: 19
  openreview_id: GCxAVXSEnO
  pdf_file: 78e11c52658d736089c251f3bf86f3d5c4059e45.pdf
  title: 'When Semantic Overlap Is Not Enough: Cross-Lingual Euphemism Transfer Between
    Turkish and English'
- abstract: 'With the recent surge in the development of large language models, the
    need for comprehensive and language-specific evaluation benchmarks has become
    critical. While significant progress has been made in evaluating English-language
    models, benchmarks for other languages, particularly those with unique linguistic
    characteristics such as Turkish, remain less developed. Our study introduces TurkBench,
    a comprehensive benchmark designed to assess the capabilities of generative large
    language models in the Turkish language. TurkBench involves 8,151 data samples
    across 21 distinct subtasks. These are organized under six main categories of
    evaluation: Knowledge, Language Understanding, Reasoning, Content Moderation,
    Turkish Grammar and Vocabulary, and Instruction Following. The diverse range of
    tasks and the culturally relevant data would provide researchers and developers
    with a valuable tool for evaluating their models and identifying areas for improvement.
    We further publish our benchmark for online submissions at https://huggingface.co/turkbench'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: ''
  authors:
  - dblp_id: https://dblp.org/pid/31/10606
    emails: '****@metu.edu.tr'
    first_name: Cagri
    google_scholar_id: https://scholar.google.com/citations?user=3iEvqKoAAAAJ
    homepage: https://www.cagritoraman.com
    institution: METU, Middle East Technical University
    last_name: Toraman
    name: Cagri Toraman
    orcid: https://orcid.org/0000-0001-6976-3258
    semantic_scholar_id: https://www.semanticscholar.org/author/Cagri-Toraman/2648640
    username: ~Cagri_Toraman1
  - emails: '****@gmail.com'
    first_name: Ahmet
    last_name: Sever
    middle_name: Kaan
    name: Ahmet Kaan Sever
    username: ~Ahmet_Kaan_Sever1
  - emails: ayseaysu.cengiz@gmail.com
    first_name: Ayşe
    middle_name: Aysu
    institution: Bilkent University
    last_name: Cengiz
    name: ayseaysu.cengiz@gmail.com
    username: ayseaysu.cengiz@gmail.com
  - emails: umutluecem@gmail.com
    first_name: Elif
    middle_name: Ecem
    institution: METU, Middle East Technical University
    last_name: Arslan
    name: umutluecem@gmail.com
    username: umutluecem@gmail.com
  - emails: gorkem.sevinc@metu.edu.tr
    first_name: Görkem
    institution: METU, Middle East Technical University
    last_name: Sevinç
    name: gorkem.sevinc@metu.edu.tr
    username: gorkem.sevinc@metu.edu.tr
  - emails: sarp.kantar@metu.edu.tr
    first_name: Sarp
    institution: METU, Middle East Technical University
    last_name: Kantar
    name: sarp.kantar@metu.edu.tr
    username: sarp.kantar@metu.edu.tr
  - emails: mete.birdal@turkcell.com.tr
    first_name: Mete
    middle_name: Mert
    institution: Turkcell AI
    last_name: Birdal
    name: mete.birdal@turkcell.com.tr
    username: mete.birdal@turkcell.com.tr
  - emails: yusuf.guldemir@turkcell.com.tr
    first_name: Yusuf
    middle_name: Faruk
    institution: NA
    last_name: Güldemir
    name: yusuf.guldemir@turkcell.com.tr
    username: yusuf.guldemir@turkcell.com.tr
  - emails: ali.kanburoglu@turkcell.com.tr
    first_name: Ali
    middle_name: Buğra
    institution: Turkcell AI
    last_name: Kanburoğlu
    name: ali.kanburoglu@turkcell.com.tr
    username: ali.kanburoglu@turkcell.com.tr
  - emails: sezen.yavan@gmail.com
    first_name: Sezen
    institution: NA
    last_name: Felekoğlu
    name: sezen.yavan@gmail.com
    username: sezen.yavan@gmail.com
  - emails: birsahin@hacettepe.edu.tr
    first_name: Birsen
    middle_name: Şahin
    institution: NA
    last_name: Kütük
    name: birsahin@hacettepe.edu.tr
    username: birsahin@hacettepe.edu.tr
  - emails: busratufan@hacettepe.du.tr
    first_name: Büşra
    institution: Hacettepe University
    last_name: Tufan
    name: busratufan@hacettepe.du.tr
    username: busratufan@hacettepe.du.tr
  - emails: gencelif@hacettepe.edu.tr
    first_name: Elif
    institution: Hacettepe University
    last_name: Genç
    name: gencelif@hacettepe.edu.tr
    username: gencelif@hacettepe.edu.tr
  - emails: '****@hacettepe.edu.tr'
    first_name: Serkan
    homepage: https://avesis.hacettepe.edu.tr/serkancoskun
    institution: Hacettepe University
    last_name: Coşkun
    name: Serkan coşkun
    username: ~Serkan_coşkun2
  - emails: gupsekind@gmail.com
    first_name: Gupse
    middle_name: Ekin
    institution: Hacettepe University
    last_name: Demir
    name: gupsekind@gmail.com
    username: gupsekind@gmail.com
  - emails: '****@std.bogazici.edu.tr'
    first_name: Muhammed
    institution: Bogazici University
    last_name: Arayıcı
    middle_name: Emin
    name: Muhammed Emin Arayıcı
    username: ~Muhammed_Emin_Arayıcı1
  - emails: '****@gmail.com'
    first_name: Olgun
    google_scholar_id: https://scholar.google.com/citations?user=URY0JI0AAAAJ&hl=en
    institution: Bogazici University
    last_name: Dursun
    name: Olgun Dursun
    username: ~Olgun_Dursun1
  - emails: '****@boun.edu.tr'
    first_name: Onur
    google_scholar_id: https://scholar.google.co.uk/citations?user=FW7c17kAAAAJ&hl=en
    homepage: http://www.cmpe.boun.edu.tr/~onur.gungor
    institution: Boğaziçi University
    last_name: Gungor
    name: Onur Gungor
    username: ~Onur_Gungor1
  - dblp_id: https://dblp.org/search?q=uskudarli
    emails: '****@boun.edu.tr'
    first_name: Susan
    google_scholar_id: https://scholar.google.com.tr/citations?user=GlcUjKIAAAAJ&hl=en
    homepage: http://www.cmpe.boun.edu.tr/~uskudarli
    institution: Boğaziçi University
    last_name: Üsküdarlı
    name: Susan Üsküdarlı
    orcid: https://orcid.org/0000-0002-0106-0182
    username: ~Susan_Üsküdarlı1
  - emails: '****@gmail.com'
    first_name: Abdullah
    homepage: https://medium.com/@abdullahtopraksoy
    institution: Istanbul University
    last_name: Topraksoy
    name: Abdullah Topraksoy
    username: ~Abdullah_Topraksoy1
  - emails: '****@metu.edu.tr'
    first_name: Esra
    homepage: https://avesis.metu.edu.tr/darici
    institution: Middle East Technical University
    last_name: Darıcı
    name: Esra Darıcı
    orcid: https://orcid.org/0000-0003-4145-9849
    username: ~Esra_Darıcı1
  decision: EACL
  file: 20.pdf
  id: 20
  openreview_id: RKN79Q25L3
  pdf_file: ac038766ad72076bf4473983e9cd3aa2b8aae8ed.pdf
  title: 'TurkBench: A Benchmark for Evaluating Turkish Large Language Models'
- abstract: Text-to-SQL systems have achieved strong performance on English benchmarks,
    yet their behavior in morphologically rich, low-resource languages remains largely
    unexplored. We introduce BIRDTurk, the first Turkish adaptation of the BIRD benchmark,
    constructed through a controlled translation pipeline that adapts schema identifiers
    to Turkish while strictly preserving the logical structure and execution semantics
    of SQL queries and databases. Translation quality is validated on a sample size
    determined by the Central Limit Theorem to ensure 95% confidence, achieving 98.15%
    accuracy on human-evaluated samples. Using BIRDTurk, we evaluate inference-based
    prompting, agentic multi-stage reasoning, and supervised fine-tuning. Our results
    reveal that Turkish introduces consistent performance degradation--driven by both
    structural linguistic divergence and underrepresentation in LLM pretraining--while
    agentic reasoning demonstrates stronger cross-lingual robustness. Supervised fine-tuning
    remains challenging for standard multilingual baselines but scales effectively
    with modern instruction-tuned models. BIRDTurk provides a controlled testbed for
    cross-lingual Text-to-SQL evaluation under realistic database conditions. We release
    the training and development splits to support future research.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: ''
  authors:
  - emails: '****@hacettepe.edu.tr'
    first_name: Burak
    google_scholar_id: https://scholar.google.com/citations?user=rjw_IMwAAAAJ&hl=tr
    last_name: Aktaş
    name: Burak Aktaş
    username: ~Burak_Aktaş2
  - emails: can.baytekin@roketsan.com.tr
    first_name: Mehmet
    middle_name: Can
    institution: Roketsan Inc., Artificial Intelligence Unit
    last_name: Baytekin
    name: Can Baytekin
    username: ~Can_Baytekin
  - emails: '****@roketsan.com.tr'
    first_name: Süha
    google_scholar_id: https://scholar.google.com/citations?user=46CjxAsAAAAJ&hl=en
    homepage: https://skagankose.com
    institution: Roketsan
    last_name: Köse
    middle_name: Kağan
    name: Süha Kağan Köse
    username: ~Süha_Kağan_Köse1
  - emails: omer.ilbilgi@roketsan.com.tr
    first_name: Ömer
    institution: Roketsan Inc., Artificial Intelligence Unit
    last_name: İlbilgi
    name: omer.ilbilgi@roketsan.com.tr
    username: omer.ilbilgi@roketsan.com.tr
  - emails: yilmaz.ozge_01@metu.edu.tr
    first_name: Elif
    middle_name: Özge
    institution: Middle East Technical University
    last_name: Yılmaz
    name: yilmaz.ozge_01@metu.edu.tr
    username: yilmaz.ozge_01@metu.edu.tr
  - dblp_id: https://dblp.org/pid/31/10606
    emails: '****@metu.edu.tr'
    first_name: Cagri
    google_scholar_id: https://scholar.google.com/citations?user=3iEvqKoAAAAJ
    homepage: https://www.cagritoraman.com
    institution: METU, Middle East Technical University
    last_name: Toraman
    name: Cagri Toraman
    orcid: https://orcid.org/0000-0001-6976-3258
    semantic_scholar_id: https://www.semanticscholar.org/author/Cagri-Toraman/2648640
    username: ~Cagri_Toraman1
  - emails: kaan.gorur@roketsan.com.tr
    first_name: Bilge
    middle_name: Kaan
    institution: Roketsan Inc., Artificial Intelligence Unit
    last_name: Görür
    name: Kaan Görür
    username: kaan.gorur@roketsan.com.tr
  decision: EACL
  file: 21.pdf
  id: 21
  openreview_id: Onov0zODsx
  pdf_file: 303e81bba9b0dc17945b1538bbafecd9e07e927a.pdf
  title: 'BIRDTurk: Adaptation of the BIRD Text-to-SQL Dataset to Turkish'
- abstract: Identifying units, 'syntactic words', for morphosyntactic analysis is
    important yet challenging for morphologically rich languages.  In this paper we
    propose a set of guiding principles to determine units of morphosyntactic  analysis,
    and apply them to the case of copular constructions in Turkic languages, in the
    context of Universal Dependencies (UD) framework.  We also provide a survey of
    the practice in the Turkic UD treebanks published to date,  and discuss the advantages
    and disadvantages of the proposed tokenisation for a selection of Turkic languages.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: ''
  authors:
  - dblp_id: https://dblp.org/pid/92/8155
    emails: '****@sfs.uni-tuebingen.de'
    first_name: Cagri
    google_scholar_id: https://scholar.google.com/citations?user=zJhEULcAAAAJ
    homepage: http://coltekin.net/cagri/
    institution: University of Tuebingen
    last_name: Coltekin
    name: Cagri Coltekin
    orcid: https://orcid.org/0000-0003-1031-6327
    semantic_scholar_id: https://www.semanticscholar.org/author/%C3%87a%C4%9Fr%C4%B1-%C3%87%C3%B6ltekin/103304646
    username: ~Cagri_Coltekin1
  - emails: '****@duck.com'
    first_name: Furkan
    last_name: Akkurt
    name: Furkan Akkurt
    orcid: https://orcid.org/0000-0002-8411-654X
    username: ~Furkan_Akkurt1
  - emails: '****@student.uni-tuebingen.de'
    first_name: Bermet
    institution: Eberhard-Karls-Universität Tübingen
    last_name: Chontaeva
    name: Bermet Chontaeva
    username: ~Bermet_Chontaeva1
  - emails: '****@student.uni-tuebingen.de'
    first_name: Soudabeh
    google_scholar_id: https://scholar.google.de/citations?hl=de&user=4g02oIcAAAAJ
    last_name: Eslami
    name: Soudabeh Eslami
    username: ~Soudabeh_Eslami1
  - dblp_id: https://dblp.org/pid/251/3997
    emails: '****@gmail.com'
    first_name: Sardana
    google_scholar_id: https://scholar.google.com/citations?user=UzgqCbYAAAAJ&hl=ru
    homepage: https://www.cs.helsinki.fi/u/sivanova/
    last_name: Ivanova
    name: Sardana Ivanova
    semantic_scholar_id: https://www.semanticscholar.org/author/153415572
    username: ~Sardana_Ivanova1
  - emails: '****@manas.edu.kg'
    first_name: Gulnura
    homepage: https://manas.edu.kg/en/humanities/synchronized_translations/0591
    institution: 'Kyrgyz-Turkish Manas University '
    last_name: Dzhumalieva
    name: Gulnura Dzhumalieva
    username: ~Gulnura_Dzhumalieva1
  - emails: '****@manas.edu.kg'
    first_name: Aida
    homepage: https://manas.edu.kg/tr/humanities/synchronized_translations/0720
    institution: Kyrgyz-Turkish Manas University and Kyrgyz-Turkish Manas University
    last_name: Kasieva
    name: Aida Kasieva
    username: ~Aida_Kasieva1
  - emails: '****@gmail.com'
    first_name: Nikolett
    homepage: https://sites.google.com/view/nikolett-mus?pli=1
    last_name: Mus
    name: Nikolett Mus
    username: ~Nikolett_Mus1
  - dblp_id: https://dblp.org/pid/126/8553
    emails: '****@swarthmore.edu'
    first_name: Jonathan
    google_scholar_id: https://scholar.google.com/citations?user=AC1uPXIAAAAJ
    homepage: http://jnw.name/
    institution: Swarthmore College
    last_name: Washington
    name: Jonathan Washington
    orcid: https://orcid.org/0000-0003-1452-8204
    semantic_scholar_id: https://www.semanticscholar.org/author/Jonathan-North-Washington/153524633
    username: ~Jonathan_Washington1
  decision: EACL
  file: 22.pdf
  id: 22
  openreview_id: 9DSuiWB6Qw
  pdf_file: e5055e11ad1742f0bb59bab6fae08822bf61cc2a.pdf
  title: Tokenisation of Turkic Copula Constructions in Universal Dependencies
- abstract: Retrieval-Augmented Generation (RAG) enhances LLM factuality, yet design
    guidance remains English-centric, limiting insights for morphologically rich languages
    like Turkish. We address this by constructing a comprehensive Turkish RAG dataset
    derived from Turkish Wikipedia and CulturaX, comprising question-answer pairs
    and relevant passage chunks. We benchmark seven stages of the RAG pipeline—from
    query transformation and reranking to answer refinement—without task-specific
    fine-tuning. Our results show that complex methods like HyDE maximize accuracy
    (85%) that is considerably higher than the baseline (78.70%). Also a Pareto-optimal
    configuration using Cross-encoder Reranking and Context Augmentation achieves
    comparable performance (84.60%) with much lower cost. We further demonstrate that
    over-stacking generative modules can degrade performance by distorting morphological
    cues, whereas simple query clarification with robust reranking offers an effective
    solution.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: ''
  authors:
  - emails: '****@roketsan.com.tr'
    first_name: Süha
    google_scholar_id: https://scholar.google.com/citations?user=46CjxAsAAAAJ&hl=en
    homepage: https://skagankose.com
    institution: Roketsan
    last_name: Köse
    middle_name: Kağan
    name: Süha Kağan Köse
    username: ~Süha_Kağan_Köse1
  - emails: can.baytekin@roketsan.com.tr
    first_name: Mehmet
    middle_name: Can
    institution: Roketsan Inc., Artificial Intelligence Unit
    last_name: Baytekin
    name: Can Baytekin
    username: ~Can_Baytekin
  - emails: '****@hacettepe.edu.tr'
    first_name: Burak
    google_scholar_id: https://scholar.google.com/citations?user=rjw_IMwAAAAJ&hl=tr
    last_name: Aktaş
    name: Burak Aktaş
    username: ~Burak_Aktaş2
  - emails: kaan.gorur@roketsan.com.tr
    first_name: Bilge
    middle_name: Kaan
    institution: Roketsan Inc., Artificial Intelligence Unit
    last_name: Görür
    name: Kaan Görür
    username: kaan.gorur@roketsan.com.tr
  - emails: '****@studenti.polito.it'
    first_name: Evren
    google_scholar_id: https://scholar.google.com/citations?user=alXZfqEAAAAJ&hl=tr&oi=ao
    last_name: Munis
    middle_name: Ayberk
    name: Evren Ayberk Munis
    username: ~Evren_Ayberk_Munis1
  - emails: '****@metu.edu.tr'
    first_name: Deniz
    institution: METU, Middle East Technical University
    last_name: Yılmaz
    name: Deniz Yılmaz
    username: ~Deniz_Yılmaz1
  - emails: '****@etu.edu.tr'
    first_name: Muhammed
    institution: Tobb Economics and Technology University
    last_name: Kartal
    middle_name: Yusuf
    name: Muhammed Yusuf Kartal
    username: ~Muhammed_Yusuf_Kartal1
  - dblp_id: https://dblp.org/pid/31/10606
    emails: '****@metu.edu.tr'
    first_name: Cagri
    google_scholar_id: https://scholar.google.com/citations?user=3iEvqKoAAAAJ
    homepage: https://www.cagritoraman.com
    institution: METU, Middle East Technical University
    last_name: Toraman
    name: Cagri Toraman
    orcid: https://orcid.org/0000-0001-6976-3258
    semantic_scholar_id: https://www.semanticscholar.org/author/Cagri-Toraman/2648640
    username: ~Cagri_Toraman1
  decision: EACL
  file: 23.pdf
  id: 23
  openreview_id: 1YmJrAO7Is
  pdf_file: 965a432f9ae8f8637fbc1cf2bb64d63fb50f9f84.pdf
  title: 'RAGTurk: Best Practices for Retrieval Augmented Generation in Turkish'
- abstract: 'Document parsing is now widely used in applications, such as large-scale
    document digitization, retrieval-augmented generation, and domain-specific pipelines
    in healthcare and education. Benchmarking these models is crucial for assessing
    their reliability and practical robustness. Existing benchmarks mostly target
    high-resource languages and provide limited coverage for low-resource settings,
    such as Turkish. Moreover, existing studies on Turkish document parsing lack a
    standardized benchmark that reflects real-world scenarios and document diversity.
    To address this gap, we introduce OCRTurk, a Turkish document parsing benchmark
    covering multiple layout elements and document categories at three difficulty
    levels. OCRTurk consists of 180 Turkish documents drawn from academic articles,
    theses, slide decks, and non-academic articles. We evaluate seven OCR models on
    OCRTurk using element-wise metrics. Across difficulty levels, PaddleOCR achieves
    the strongest overall results, leading most element-wise metrics except figures
    and attaining the best Normalized Edit Distance scores in easy, medium, and hard
    subsets. We also observe performance variation by document type: models perform
    well on non-academic documents, while slideshows become the most challenging.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: ''
  authors:
  - emails: '****@metu.edu.tr'
    first_name: Deniz
    institution: METU, Middle East Technical University
    last_name: Yılmaz
    name: Deniz Yılmaz
    username: ~Deniz_Yılmaz1
  - emails: '****@studenti.polito.it'
    first_name: Evren
    google_scholar_id: https://scholar.google.com/citations?user=alXZfqEAAAAJ&hl=tr&oi=ao
    last_name: Munis
    middle_name: Ayberk
    name: Evren Ayberk Munis
    username: ~Evren_Ayberk_Munis1
  - dblp_id: https://dblp.org/pid/31/10606
    emails: '****@metu.edu.tr'
    first_name: Cagri
    google_scholar_id: https://scholar.google.com/citations?user=3iEvqKoAAAAJ
    homepage: https://www.cagritoraman.com
    institution: METU, Middle East Technical University
    last_name: Toraman
    name: Cagri Toraman
    orcid: https://orcid.org/0000-0001-6976-3258
    semantic_scholar_id: https://www.semanticscholar.org/author/Cagri-Toraman/2648640
    username: ~Cagri_Toraman1
  - emails: '****@roketsan.com.tr'
    first_name: Süha
    google_scholar_id: https://scholar.google.com/citations?user=46CjxAsAAAAJ&hl=en
    homepage: https://skagankose.com
    institution: Roketsan
    last_name: Köse
    middle_name: Kağan
    name: Süha Kağan Köse
    username: ~Süha_Kağan_Köse1
  - emails: '****@hacettepe.edu.tr'
    first_name: Burak
    google_scholar_id: https://scholar.google.com/citations?user=rjw_IMwAAAAJ&hl=tr
    last_name: Aktaş
    name: Burak Aktaş
    username: ~Burak_Aktaş2
  - emails: can.baytekin@roketsan.com.tr
    first_name: Mehmet
    middle_name: Can
    institution: Roketsan Inc., Artificial Intelligence Unit
    last_name: Baytekin
    name: Can Baytekin
    username: ~Can_Baytekin
  - emails: kaan.gorur@roketsan.com.tr
    first_name: Bilge
    middle_name: Kaan
    institution: Roketsan Inc., Artificial Intelligence Unit
    last_name: Görür
    name: Kaan Görür
    username: kaan.gorur@roketsan.com.tr
  decision: EACL
  file: 24.pdf
  id: 24
  openreview_id: DbYKEWsIE0
  pdf_file: 4423ddeeddabe3bef8d87ee9036afd9963474053.pdf
  title: 'OCRTurk: A Comprehensive OCR Benchmark for Turkish'
- abstract: Large Language Models (LLMs) achieve strong performance on many tasks,
    but they still struggle with morphologically rich, low-resource languages such
    as Turkish. This difficulty stems from Turkish being an agglutinative language
    and underrepresented in multilingual training data, which causes current models
    to often fail at capturing its morphology, flexible word order, and formal registers.
    In this paper, we introduce MODA (Model Adapted for Domain Applications), a Turkish-specialized
    LLM built via a modular pipeline that combines continual pre-training, parameter-efficient
    fine-tuning, and model merging. Starting from Qwen2.5-7B as the base model, we
    first perform large-scale continual pre-training on a Turkish web corpus to improve
    grammatical and morphological representations. We then apply parameter-efficient
    supervised fine-tuning on task-oriented instruction data, and finally merge specialized
    variants into a single unified model. We evaluate MODA on TurkishMMLU, the Turkish
    subset of EXAMS, and TRCLAIM-19, where it consistently outperforms both the base
    and instruction-tuned Qwen2.5-7B models. Our results support a training strategy
    that explicitly separates linguistic acquisition from task alignment when adapting
    LLMs to morphologically rich, underrepresented languages under realistic hardware
    constraints.
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: ''
  authors:
  - emails: '****@gmail.com'
    first_name: Alperen
    google_scholar_id: https://scholar.google.com/citations?user=6xTzJSUAAAAJ&hl=en
    last_name: Bayar
    middle_name: Enes
    name: Alperen Enes Bayar
    orcid: https://orcid.org/0009-0001-8921-6179
    username: ~Alperen_Enes_Bayar1
  - emails: '****@gmail.com'
    first_name: Mert
    google_scholar_id: https://scholar.google.com/citations?user=3AHmt5AAAAAJ&hl=en
    last_name: Ege
    name: Mert Ege
    username: ~Mert_Ege1
  - emails: '****@gmail.com'
    first_name: Gökhan
    institution: Atilim University
    last_name: Yurtalan
    name: Gökhan Yurtalan
    orcid: https://orcid.org/0000-0003-4741-2136
    username: ~Gökhan_Yurtalan1
  - emails: '****@ceng.metu.edu.tr'
    first_name: Alper
    google_scholar_id: https://scholar.google.com/citations?user=UzSfT68AAAAJ&hl=en&oi=ao
    last_name: Karamanlioglu
    name: Alper Karamanlioglu
    orcid: https://orcid.org/0000-0001-5536-1354
    username: ~Alper_Karamanlioglu1
  - dblp_id: https://dblp.org/pid/200/8147
    emails: '****@metu.edu.tr'
    first_name: Berkan
    google_scholar_id: https://scholar.google.com/citations?user=Pv5y05wAAAAJ&hl=en&oi=ao
    institution: Middle East Technical University
    last_name: Demirel
    name: Berkan Demirel
    username: ~Berkan_Demirel1
  - dblp_id: https://dblp.org/pid/54/2808
    emails: '****@gmail.com'
    first_name: Ramazan Gokberk
    google_scholar_id: https://scholar.google.it/citations?user=Za7uka8AAAAJ&hl=en&oi=ao
    homepage: http://user.ceng.metu.edu.tr/~gcinbis/
    institution: Middle East Technical University
    last_name: Cinbis
    name: Ramazan Gokberk Cinbis
    username: ~Ramazan_Gokberk_Cinbis1
  decision: EACL
  file: 25.pdf
  id: 25
  openreview_id: FlFopdiWnx
  pdf_file: 1a1900e5d412c056da2f54eaf5af4b3dea0be70f.pdf
  title: Building a Turkish Large Language Model via Continual Pre-Training and Parameter-Efficient
    Adaptation
- abstract: 'Light verb constructions (LVCs) are a challenging class of verbal multiword
    expressions, especially in Turkish,

    where rich morphology and productive complex predicates create minimal contrasts
    between idiomatic predicate

    meanings and literal verb--argument uses. This paper asks what signals drive LVC
    classification by

    systematically restricting model inputs. Using UD-derived supervision, we compare
    lemma-driven baselines

    (lemma TF--IDF + Logistic Regression; BERTurk trained on lemma sequences), a grammar-only
    Logistic Regression

    over UD morphosyntax (UPOS/DEPREL/MORPH), and a full-input BERTurk baseline. We
    evaluate on a controlled

    diagnostic set with Random negatives, lexical controls (NLVC), and LVC positives,
    reporting split-wise

    performance to expose decision-boundary behavior. Results show that coarse morphosyntax
    alone is insufficient

    for robust LVC detection under controlled contrasts, while lexical identity supports
    LVC judgments but is

    sensitive to calibration and normalization choices. Overall, our findings motivate
    targeted evaluation for

    Turkish MWEs and highlight that “lemma-only” is not a single representation but
    depends critically on how

    normalization is instantiated.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: ''
  authors:
  - emails: '****@uchicago.edu'
    first_name: Sercan
    homepage: https://linguistics.uchicago.edu/people/sercan-karakas
    institution: University of Chicago
    last_name: Karakas
    name: Sercan Karakas
    username: ~Sercan_Karakas1
  - emails: '****@firat.edu.tr'
    first_name: Yusuf
    last_name: Şimşek
    name: Yusuf Şimşek
    username: ~Yusuf_Şimşek1
  decision: EACL
  file: 26.pdf
  id: 26
  openreview_id: lgmLFsEVoP
  pdf_file: c1a8e594600e779f9367b1b3757c5d9e0b9a9555.pdf
  title: 'From Lemmas to Dependencies: What Signals Drive Light Verbs Classification?'
- abstract: 'Current interpretability methods for Large Language Models (LLMs) operate
    on a fundamental yet flawed assumption: that subword tokens represent independent
    semantic units. We prove that this assumption creates a fidelity bottleneck in
    Morphologically Rich Languages (MRLs), where semantic meaning is densely encoded
    in sub-token morphemes. We term this phenomenon the Tokenization-Morphology Misalignment
    (TMM). To resolve TMM, we introduce MAFEX (Morpheme-Aligned Faithful Explanations),
    a theoretically grounded framework that redefines feature attribution as a linear
    projection from the computational (token) basis to the linguistic (morpheme) basis.
    We evaluate our method on a diverse suite of Turkish LLMs, including BERTurk,
    BERTurk-Sentiment, Cosmos-BERT, and Kumru-2B. On our embedded benchmark (N=20),
    MAFEX achieves an average F1@1 of 91.25% compared to 13.75% for standard token-level
    baselines (IG, SHAP, DeepLIFT), representing a +77.5% absolute improvement, establishing
    it as the new standard for faithful multilingual interpretability.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: ''
  authors:
  - emails: '****@gmail.com'
    first_name: Muhammet
    google_scholar_id: https://scholar.google.com/citations?user=KDqGjjYAAAAJ&hl=en
    last_name: Yagiz
    middle_name: Anil
    name: Muhammet Anil Yagiz
    username: ~Muhammet_Anil_Yagiz1
  - emails: '****@kku.edu.tr'
    first_name: Fahrettin
    google_scholar_id: https://scholar.google.com/citations?user=hFzCE0MAAAAJ&hl
    homepage: https://kariyer.kku.edu.tr/akademik/default.aspx?sicil=a-3618
    institution: Kırıkkale University
    last_name: Horasan
    name: Fahrettin Horasan
    orcid: https://orcid.org/0000-0003-4554-9083
    username: ~Fahrettin_Horasan1
  decision: EACL
  file: 27.pdf
  id: 27
  openreview_id: 6bMsFMyfc9
  pdf_file: 8315b7f77e9f0350895f613436d6f85b2b8348dd.pdf
  title: 'Beyond the Token: Correcting the Tokenization Bias in XAI via Morphologically-Aligned
    Projection'
- abstract: 'This paper presents an overview of the SIGTURK 2026 Shared Task on Terminology-Aware Machine Translation for English-Turkish Scientific Texts. We address the critical challenge of terminological accuracy in low-resource settings by constructing the first terminology-rich English-Turkish parallel corpus, comprising 3,300 sentence pairs from STEM domains with 10,157 expert-validated term pairs. The shared task consists of three subtasks: term detection, expert-guided correction, and end-to-end post-editing. We evaluate state-of-the-art baselines (including GPT-5.2 and Claude Sonnet 4.5) alongside participant systems employing diverse strategies from fine-tuning to Retrieval-Augmented Generation (RAG). Our results highlight that while massive generalist models dominate zero-shot detection, smaller, domain-adapted models using Supervised Fine-Tuning and Reinforcement Learning can significantly outperform them in end-to-end post-editing. Furthermore, we find that rigid retrieval pipelines often disrupt fluency, whereas Chain-of-Thought prompting allows models to integrate terminology more naturally. Despite these advances, a significant gap remains between automated systems and human expert performance in strict terminology correction.'
  attributes:
    paper_type: N/A
    presentation_type: N/A
    submitted_area: ''
  authors:
  - emails: '****@gmail.com'
    first_name: Ali
    last_name: Gebeşçe
  - emails: '****@gmail.com'
    first_name: Abdulfattah
    last_name: Safa
  - emails: '****@gmail.com'
    first_name: Ege
    middle_name: Uğur
    last_name: Amasya
  - emails: '****@gmail.com'
    first_name: Gözde
    middle_name: Gül
    last_name: Şahin
  decision: EACL
  file: 28.pdf
  id: 28
  title: 'Overview of the SIGTURK 2026 Shared Task: Terminology-Aware Machine Translation for English–Turkish Scientific Texts'
